---
layout: post
title:  "AirCapRL: Autonomous Aerial Human Motion Capture using DRL"
date:   2022-07-10 20:00
category: robotics
icon: git
keywords: multidrone
image: AirCapRL.png
preview: 1
---

# AirCapRL: Autonomous Aerual Human Motion Capture using Deep Reinforcement Learning
[AirCapRL]: https://arxiv.org/abs/2007.06343	"Paper Link"

## Goal

**Estimate the trajectory of body pose and shape of a single moving person using multiple micro aerial vehicles**



## Problem Formation

- Sequential decision making task to achieve the vision-based motion capture objectives
- Solving it using a deep neural network-based RL method
- Leverage proximal policy optimization (PPO) to train a stochastic decentralized control policy for formation control

## Novelty

- Taking a learning-based approach to map and embed the dependency (3D pose and shape estimation are dependent on viewpoints of the MAVs) within the formation control algorithm.
- Addressing the formation control for aerial MoCap as a multi-agent reinforcement learning (RL) problem, without making any assumptions on the observation model.



## Methodology

- Train two different kinds of agents, and hence, network.
  - A single agent with only MoCap objectives
    - learn to follow the person and orient itself in the direction of the person in order to achieve accurate MoCap from the back-end estimator
  - Multi-agents with both MoCap and collision avoidance objectives
    - learn how to avoid each other and distribute themselves around the person to cover all possible viewpoints.

### Network 1: Single Agent Network

1. Only Centering Reward
2. SPIN Reward
3. Weighted SPIN Reward
4. Cenetring and Weighted SPIN Reward

### Network 2: Multi-agent Network:

1. Centering, collision avoidance and AlphaPose Triangulation Reward (Trained with Static Subject)
2. Centerting, colision avoideance and Multiview HMR Reward (Trained with Static Subject )
3. Centering, continuous collision avoidance and Mutiview HMR Reward (Trained with Moving Subject)
4. Centering and Multiview HMR Reward (Trained with Moving Subject) + Potential Field


<img width="957" alt="截屏2022-07-10 18 28 53" src="https://user-images.githubusercontent.com/77670073/178164998-65f29802-e479-4f8e-af99-fdcaecd9a5b8.png">


## Conclusion

1. Demonstrate that in terms of accuracy, DRL-based approach is on par with the state-of-the-art MPC-based approach
2. Network 1.4 and 1.2 are the two most successful approaches for the MoCap task
3. the centering the person in the image does not have a great impact on the accuracy of the motion capture estimates
4. Network 1.2, 1.3, and 1.4 never lose the person from the camera FOV
5. Network 2.3 is very simialr to the MPC-based method in terms of the MPE median value and has much less MPE variance than MPC

**Overcome the need for knowing models, strategies as well as any ad-hoc collision avoidance techniques**



#### Interesting question

strategy based method has drawback like losing the person from the field of view.
